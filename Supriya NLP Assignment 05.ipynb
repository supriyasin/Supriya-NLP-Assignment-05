{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f183040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What are Sequence-to-sequence models?\n",
    "\n",
    "\"\"\"Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture commonly used \n",
    "for tasks involving sequences, such as machine translation, text summarization, and speech recognition.\n",
    "The key idea behind Seq2Seq models is that they can take variable-length sequences as input and produce \n",
    "variable-length sequences as output.\n",
    "\n",
    "The architecture typically consists of two main components: an encoder and a decoder.\n",
    "\n",
    "1. **Encoder**: The encoder processes the input sequence and converts it into a fixed-size context\n",
    "vector, which contains the encoded representation of the input sequence. This context vector captures\n",
    "the semantic meaning of the input sequence and serves as the starting point for generating the output\n",
    "sequence.\n",
    "\n",
    "2. **Decoder**: The decoder takes the context vector produced by the encoder and generates the output\n",
    "sequence one token at a time. At each time step, the decoder produces the next token in the output\n",
    "sequence based on its current hidden state and the previously generated tokens. The decoder's \n",
    "hidden state is updated recurrently as it generates the output sequence.\n",
    "\n",
    "Seq2Seq models are typically trained using pairs of input-output sequences, with the objective\n",
    "of minimizing the difference between the model's predicted output sequences and the target output \n",
    "sequences. This is often done using techniques such as teacher forcing, where the decoder is fed \n",
    "the target output tokens during training to guide its learning process.\n",
    "\n",
    "Seq2Seq models have been instrumental in advancing the state-of-the-art in various natural language \n",
    "processing tasks, and they continue to be an active area of research and development in the field.\"\"\"\n",
    "\n",
    "#2. What are the Problem with Vanilla RNNs?\n",
    "\n",
    "\"\"\"Vanilla recurrent neural networks (RNNs) suffer from several limitations that can make them \n",
    "challenging to train effectively, especially for tasks involving long sequences and capturing\n",
    "long-range dependencies. Some of the main problems with vanilla RNNs include:\n",
    "\n",
    "1. **Vanishing and Exploding Gradients**: Vanilla RNNs are prone to the vanishing and exploding \n",
    "gradient problem, where gradients either become too small (vanish) or too large (explode) as they \n",
    "are backpropagated through time. This issue arises because gradients are multiplied at each time \n",
    "step during backpropagation, leading to exponential growth or decay. As a result, the network \n",
    "struggles to learn long-range dependencies and may fail to capture information from distant past time steps.\n",
    "\n",
    "2. **Short-Term Memory**: Vanilla RNNs have a limited capacity to retain information over long \n",
    "sequences. Due to the vanishing gradient problem, the network's ability to remember information\n",
    "from earlier time steps diminishes as the sequence length increases. This limitation hinders their \n",
    "performance on tasks that require capturing long-term dependencies, such as language modeling and\n",
    "machine translation.\n",
    "\n",
    "3. **Difficulty in Capturing Contextual Information**: Vanilla RNNs treat all time steps equally\n",
    "and do not have mechanisms for selectively attending to relevant parts of the input sequence. \n",
    "As a result, they may struggle to focus on important contextual information and may be easily\n",
    "influenced by noise or irrelevant input.\n",
    "\n",
    "4. **Difficulty in Capturing Sequential Patterns**: Vanilla RNNs have difficulty capturing complex \n",
    "sequential patterns, especially those that involve non-linear transformations or dependencies between\n",
    "distant time steps. This limitation can affect their performance on tasks such as sequence prediction\n",
    "and time series forecasting.\n",
    "\n",
    "5. **Training Instability**: Due to the vanishing and exploding gradient problem, training vanilla RNNs\n",
    "can be unstable, especially when using gradient-based optimization algorithms such as stochastic\n",
    "gradient descent (SGD). Unstable training can result in slow convergence, poor generalization, and \n",
    "difficulty in finding optimal model parameters.\n",
    "\n",
    "To address these limitations, various advanced architectures and training techniques have been developed, \n",
    "such as Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs), and attention mechanisms. \n",
    "These approaches aim to overcome the shortcomings of vanilla RNNs and improve their performance on tasks\n",
    "involving sequential data.\"\"\"\n",
    "\n",
    "#3. What is Gradient clipping?\n",
    "\n",
    "\"\"\"Gradient clipping is a technique used during the training of neural networks to mitigate the \n",
    "exploding gradient problem, which occurs when gradients become excessively large during backpropagation.\n",
    "This phenomenon can lead to numerical instability and hinder the convergence of the training process.\n",
    "\n",
    "Gradient clipping involves capping the norm of the gradients to a predefined threshold value before \n",
    "updating the model parameters. This threshold can be chosen based on experimentation or domain\n",
    "knowledge and is typically set to a value that prevents gradients from growing too large while\n",
    "still allowing for effective learning.\n",
    "\n",
    "The basic idea behind gradient clipping is to monitor the norm of the gradients computed during \n",
    "backpropagation. If the norm exceeds the predefined threshold, the gradients are scaled down\n",
    "proportionally to ensure that their norm remains within the specified range. This prevents the\n",
    "gradients from becoming too large and helps stabilize the training process.\n",
    "\n",
    "Gradient clipping can be applied at different levels of granularity:\n",
    "\n",
    "1. **Global Gradient Clipping**: In this approach, the norm of the entire gradient vector across \n",
    "all parameters of the neural network is computed. If this norm exceeds the threshold, all gradients\n",
    "are scaled down uniformly to ensure that the norm remains below the threshold.\n",
    "\n",
    "2. **Layer-wise Gradient Clipping**: Instead of computing the norm of the entire gradient vector,\n",
    "the norm of gradients is computed separately for each layer of the neural network. This allows for\n",
    "more fine-grained control over gradient scaling, as each layer can have its own threshold value.\n",
    "\n",
    "3. **Element-wise Gradient Clipping**: In some cases, it may be beneficial to clip gradients \n",
    "element-wise, meaning that each individual gradient value is scaled down if it exceeds a certain\n",
    "threshold. This approach can be useful when dealing with sparse gradients or when specific parameters \n",
    "need to be constrained.\n",
    "\n",
    "Gradient clipping is a simple yet effective technique for stabilizing the training of neural networks,\n",
    "especially recurrent neural networks (RNNs) and deep neural networks (DNNs), where the exploding \n",
    "gradient problem is more prevalent. By preventing gradients from becoming too large, gradient clipping \n",
    "helps improve the convergence of the training process and facilitates better generalization performance\n",
    "of the model.\"\"\"\n",
    "\n",
    "#4. Explain Attention mechanism\n",
    "\n",
    "\"\"\"The attention mechanism is a key component in many sequence-to-sequence (Seq2Seq) models, particularly\n",
    "in tasks involving variable-length input and output sequences, such as machine translation, text summarization,\n",
    "and speech recognition. It allows the model to focus on different parts of the input sequence when generating \n",
    "each part of the output sequence, enabling the model to capture relevant contextual information effectively.\n",
    "\n",
    "At a high level, the attention mechanism works by associating weights with each element of the input sequence,\n",
    "indicating its importance or relevance to the generation of the current output token. These weights are\n",
    "dynamically computed based on the similarity between the current decoder state and each encoder state, \n",
    "reflecting the alignment between the input and output sequences.\n",
    "\n",
    "The attention mechanism typically consists of the following components:\n",
    "\n",
    "1. **Encoder**: The encoder processes the input sequence and produces a set of encoder states or\n",
    "representations, often using recurrent neural networks (RNNs), convolutional neural networks (CNNs),\n",
    "or transformer architectures. Each encoder state captures information about a specific part of the \n",
    "input sequence.\n",
    "\n",
    "2. **Decoder**: The decoder generates the output sequence one token at a time. At each time step, \n",
    "the decoder computes an attention distribution over the encoder states, indicating how much attention\n",
    "to allocate to each part of the input sequence. This attention distribution is then used to compute a\n",
    "context vector, which is a weighted sum of the encoder states, representing the information relevant \n",
    "to the current decoding step.\n",
    "\n",
    "3. **Attention Mechanism**: The attention mechanism computes the attention weights dynamically based on\n",
    "the similarity between the current decoder state and each encoder state. Various methods can be used to\n",
    "calculate these weights, such as dot product attention, additive attention, or multiplicative attention. \n",
    "Once the attention weights are computed, they are used to weigh the encoder states, producing the context vector.\n",
    "\n",
    "4. **Context Vector**: The context vector is a weighted sum of the encoder states, where the weights are\n",
    "determined by the attention distribution. It represents the information from the input sequence that is \n",
    "most relevant to the current decoding step. The context vector is then concatenated with the input to the\n",
    "decoder at each time step to influence the generation of the output token.\n",
    "\n",
    "By incorporating the attention mechanism, Seq2Seq models can effectively capture long-range dependencies,\n",
    "handle variable-length input and output sequences, and improve the quality of generated sequences. \n",
    "The attention mechanism has become a fundamental building block in many state-of-the-art neural network\n",
    "architectures for sequence modeling tasks.\"\"\"\n",
    "\n",
    "#5. Explain Conditional random fields (CRFs)\n",
    "\n",
    "\"\"\"Conditional Random Fields (CRFs) are a type of probabilistic graphical model used for modeling \n",
    "sequential data, particularly in tasks such as sequence labeling, named entity recognition, \n",
    "part-of-speech tagging, and speech recognition. CRFs model the conditional probability distribution \n",
    "of label sequences given input observations, allowing them to capture dependencies between adjacent \n",
    "labels in a sequence.\n",
    "\n",
    "Here's how CRFs work:\n",
    "\n",
    "1. **Problem Formulation**: In sequence labeling tasks, the goal is to assign a label to each element \n",
    "in a sequence of observations. For example, in part-of-speech tagging, the input sequence consists of \n",
    "words in a sentence, and the task is to assign a part-of-speech tag to each word.\n",
    "\n",
    "2. **Feature Extraction**: Before training a CRF model, relevant features are extracted from the input \n",
    "observations and potential labels. These features can include word identities, word context, linguistic \n",
    "features, and any other information that may be informative for predicting the labels. Features are\n",
    "typically defined over windows of observations and labels.\n",
    "\n",
    "3. **Model Representation**: In CRFs, the joint probability of label sequences \\( y \\) given input\n",
    "observations \\( x \\) is modeled using a log-linear model:\n",
    "\n",
    "\\[\n",
    "P(y \\mid x) = \\frac{1}{Z(x)} \\exp \\left( \\sum_{t=1}^{T} \\sum_{i} \\lambda_i f_i(y_{t-1}, y_t, x, t) \\right)\n",
    "\\]\n",
    "\n",
    "where:\n",
    "   - \\( y \\) is a label sequence,\n",
    "   - \\( x \\) is an input observation sequence,\n",
    "   - \\( T \\) is the length of the sequence,\n",
    "   - \\( f_i \\) are feature functions that capture the relevance of features for predicting label transitions,\n",
    "   - \\( \\lambda_i \\) are weights associated with each feature function,\n",
    "   - \\( Z(x) \\) is the normalization factor (partition function) that ensures the probabilities sum up to 1.\n",
    "\n",
    "4. **Training**: The parameters \\( \\lambda_i \\) of the CRF model are learned from labeled training data\n",
    "using optimization algorithms such as gradient descent or L-BFGS. The objective is typically to maximize\n",
    "the conditional log-likelihood of the labeled training data.\n",
    "\n",
    "5. **Inference**: Given an input observation sequence \\( x \\), the most probable label sequence \\( y^* \\)\n",
    "is inferred by maximizing the conditional probability \\( P(y \\mid x) \\). This can be efficiently computed\n",
    "using dynamic programming algorithms such as the Viterbi algorithm.\n",
    "\n",
    "CRFs have several advantages, including their ability to model complex label dependencies, handle\n",
    "overlapping and non-local features, and incorporate rich feature representations. They have been widely \n",
    "used in natural language processing and other sequence modeling tasks, often achieving state-of-the-art\n",
    "performance when combined with appropriate feature representations and training techniques. However,\n",
    "CRFs can be computationally expensive to train and may require careful feature engineering to achieve\n",
    "optimal performance.\"\"\"\n",
    "\n",
    "#6. Explain self-attention\n",
    "\n",
    "\"\"\"Self-attention is a mechanism used in neural network architectures, particularly in transformer models, \n",
    "to capture relationships between different positions within a sequence. It enables the model to weigh the\n",
    "importance of each element in the sequence when processing each element, allowing for more effective \n",
    "encoding of long-range dependencies and capturing of contextual information.\n",
    "\n",
    "Here's how self-attention works:\n",
    "\n",
    "1. **Input Representation**: Before applying self-attention, the input sequence is typically \n",
    "embedded into a sequence of vectors. Each vector represents an element (e.g., word or token) \n",
    "in the input sequence. These vectors can be learned through an embedding layer or obtained \n",
    "using pre-trained word embeddings.\n",
    "\n",
    "2. **Key, Query, and Value Representations**: In self-attention, each input vector is transformed \n",
    "into three vectors: key (\\( K \\)), query (\\( Q \\)), and value (\\( V \\)) representations. \n",
    "These transformations are usually linear projections followed by non-linear activation functions. \n",
    "Mathematically, for an input vector \\( x_i \\), the key (\\( k_i \\)), query (\\( q_i \\)), and value \n",
    "(\\( v_i \\)) representations are computed as follows:\n",
    "\n",
    "   \\[\n",
    "   k_i = W_k x_i, \\quad q_i = W_q x_i, \\quad v_i = W_v x_i\n",
    "   \\]\n",
    "\n",
    "   where \\( W_k \\), \\( W_q \\), and \\( W_v \\) are learnable weight matrices.\n",
    "\n",
    "3. **Attention Weights**: After obtaining key (\\( K \\)), query (\\( Q \\)), and value (\\( V \\))\n",
    "representations for each input vector, attention weights are computed to capture the importance\n",
    "of each element in the sequence with respect to the current element. The attention weight (\\( \\alpha_{ij} \\)) \n",
    "between elements \\( i \\) and \\( j \\) is computed as the softmax of the dot product of their query and key \n",
    "representations:\n",
    "\n",
    "   \\[\n",
    "   \\alpha_{ij} = \\text{softmax} \\left( \\frac{{q_i \\cdot k_j}}{\\sqrt{d_k}} \\right)\n",
    "   \\]\n",
    "\n",
    "   where \\( d_k \\) is the dimensionality of the key and query vectors.\n",
    "\n",
    "4. **Context Vector**: Using the computed attention weights, a context vector (\\( c_i \\)) for\n",
    "each element in the sequence is obtained by taking a weighted sum of the value representations:\n",
    "\n",
    "   \\[\n",
    "   c_i = \\sum_{j} \\alpha_{ij} v_j\n",
    "   \\]\n",
    "\n",
    "   This context vector captures information from other elements in the sequence, with the amount \n",
    "   of information determined by the attention weights.\n",
    "\n",
    "5. **Multi-head Attention**: To enhance the representational capacity and capture different aspects\n",
    "of relationships in the sequence, self-attention is often performed in parallel with multiple sets \n",
    "of key, query, and value transformations, called attention heads. The context vectors from different \n",
    "attention heads are concatenated and linearly transformed to produce the final output of the self-attention\n",
    "layer.\n",
    "\n",
    "Self-attention has become a crucial component in transformer-based architectures, such as BERT, GPT,\n",
    "and T5, enabling these models to effectively capture dependencies between elements in long sequences \n",
    "and achieve state-of-the-art performance in various natural language processing tasks, including \n",
    "language understanding, generation, and translation.\"\"\"\n",
    "\n",
    "#7. What is Bahdanau Attention?\n",
    "\n",
    "\"\"\"Bahdanau attention, also known as additive attention, is an attention mechanism introduced by \n",
    "Dzmitry Bahdanau et al. in the paper \"Neural Machine Translation by Jointly Learning to Align and\n",
    "Translate\" in 2014. It is a type of attention mechanism commonly used in sequence-to-sequence models, \n",
    "particularly in tasks such as machine translation, where the model needs to align input and output\n",
    "sequences effectively.\n",
    "\n",
    "Unlike the more traditional dot-product attention, which computes attention weights based solely on \n",
    "the similarity between the query and key vectors, Bahdanau attention calculates attention scores by \n",
    "combining the query and key vectors using a learned alignment model. This allows the attention mechanism \n",
    "to learn to focus on different parts of the input sequence for each output token, capturing more nuanced\n",
    "relationships between them.\n",
    "\n",
    "Here's how Bahdanau attention works:\n",
    "\n",
    "1. **Key, Query, and Value Representations**: Similar to other attention mechanisms, Bahdanau attention \n",
    "first computes key (\\( K \\)), query (\\( Q \\)), and value (\\( V \\)) representations for each element in \n",
    "the input sequence. These representations are typically obtained through linear transformations of the \n",
    "input embeddings.\n",
    "\n",
    "2. **Alignment Model**: Bahdanau attention introduces an alignment model, which is a small feedforward\n",
    "neural network that takes the previous hidden state of the decoder (query) and the current hidden state\n",
    "of the encoder (key) as inputs. This alignment model produces a context vector, or alignment score,\n",
    "representing the relevance of each encoder hidden state to the current decoding step.\n",
    "\n",
    "3. **Attention Scores**: Using the alignment model, attention scores (\\( e_{ij} \\)) are computed for\n",
    "each pair of encoder hidden states (\\( h_j \\)) and the current decoder hidden state (\\( s_{i-1} \\)).\n",
    "These attention scores indicate how well each encoder hidden state aligns with the current decoding step.\n",
    "\n",
    "   \\[\n",
    "   e_{ij} = \\text{alignment\\_model}(s_{i-1}, h_j)\n",
    "   \\]\n",
    "\n",
    "4. **Attention Weights**: The attention scores are then normalized across all encoder hidden states\n",
    "using the softmax function to obtain attention weights (\\( \\alpha_{ij} \\)).\n",
    "\n",
    "   \\[\n",
    "   \\alpha_{ij} = \\frac{{\\exp(e_{ij})}}{{\\sum_{k} \\exp(e_{ik})}}\n",
    "   \\]\n",
    "\n",
    "5. **Context Vector**: Finally, the context vector (\\( c_i \\)) is computed as a weighted sum of the \n",
    "encoder hidden states, where the weights are determined by the attention weights.\n",
    "\n",
    "   \\[\n",
    "   c_i = \\sum_{j} \\alpha_{ij} h_j\n",
    "   \\]\n",
    "\n",
    "Bahdanau attention allows the model to dynamically align input and output sequences at each decoding \n",
    "step, enabling it to focus on different parts of the input sequence as needed for generating each \n",
    "output token. This flexibility makes Bahdanau attention particularly effective for tasks involving\n",
    "variable-length input and output sequences, such as machine translation and text summarization.\"\"\"\n",
    "\n",
    "#8. What is a Language Model?\n",
    "\n",
    "\"\"\"A language model is a statistical model that is capable of predicting the probability of a sequence \n",
    "of words or tokens in a natural language. It learns the structure and patterns of a language from a \n",
    "corpus of text data and uses this knowledge to generate or evaluate new sequences of words.\n",
    "\n",
    "Language models are fundamental in natural language processing (NLP) and have numerous applications, \n",
    "including:\n",
    "\n",
    "1. **Text Generation**: Language models can generate coherent and contextually relevant text based \n",
    "on a given prompt or seed text. They are used in applications such as chatbots, virtual assistants, \n",
    "and content generation.\n",
    "\n",
    "2. **Machine Translation**: Language models play a crucial role in machine translation systems by\n",
    "modeling the probability distribution of translations from one language to another. They help generate\n",
    "fluent and accurate translations by considering the context of the source and target languages.\n",
    "\n",
    "3. **Speech Recognition**: In speech recognition systems, language models are used to predict the most \n",
    "likely sequence of words given an input speech signal. They help improve the accuracy of transcription \n",
    "by incorporating linguistic constraints and context.\n",
    "\n",
    "4. **Spell Checking and Correction**: Language models can be used to detect and correct spelling errors \n",
    "by predicting the most likely correct word based on the context of the surrounding words.\n",
    "\n",
    "5. **Language Understanding**: Language models aid in tasks such as sentiment analysis, named entity \n",
    "recognition, and text classification by capturing semantic and syntactic patterns in text data.\n",
    "\n",
    "Language models can be categorized based on the type of data they are trained on and the approach used \n",
    "for modeling:\n",
    "\n",
    "1. **Count-Based Models**: These models estimate the probability of word sequences based on the frequency \n",
    "of occurrence of words and sequences of words in a corpus. Examples include N-gram models and Hidden Markov \n",
    "Models (HMMs).\n",
    "\n",
    "2. **Neural Language Models**: These models use neural networks to learn distributed representations of words \n",
    "and capture complex patterns in text data. Examples include feedforward neural networks, recurrent neural\n",
    "networks (RNNs), long short-term memory (LSTM) networks, and transformer-based models like GPT (Generative\n",
    "Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "Language models have evolved significantly over the years, with neural language models, in particular, \n",
    "achieving remarkable performance improvements on a wide range of NLP tasks. They continue to be a key\n",
    "area of research and development in the field of natural language processing.\"\"\"\n",
    "\n",
    "#9. What is Multi-Head Attention?\n",
    "\n",
    "\"\"\"Multi-head attention is a variant of the self-attention mechanism used in transformer-based neural \n",
    "network architectures, introduced in the \"Attention is All You Need\" paper by Vaswani et al. in 2017.\n",
    "It enhances the representational capacity of self-attention by allowing the model to jointly attend to \n",
    "different subspaces of the input representation in parallel.\n",
    "\n",
    "In multi-head attention, the self-attention mechanism is applied multiple times, each time using different\n",
    "linear projections of the input vectors to compute key, query, and value representations. This results in \n",
    "multiple sets of attention scores and context vectors, known as attention heads. The outputs of these \n",
    "attention heads are then concatenated and linearly transformed to produce the final output of the multi-head \n",
    "attention layer.\n",
    "\n",
    "Here's how multi-head attention works:\n",
    "\n",
    "1. **Key, Query, and Value Representations**: The input sequence is first transformed into key (\\( K \\)),\n",
    "query (\\( Q \\)), and value (\\( V \\)) representations using learnable linear projections. These projections \n",
    "are typically parameterized by weight matrices \\( W_k \\), \\( W_q \\), and \\( W_v \\), respectively.\n",
    "\n",
    "2. **Multi-Head Attention Calculation**: The key, query, and value representations are split into multiple \n",
    "heads, each representing a different subspace of the input representation. For each head, attention scores \n",
    "are computed independently using the dot product of the query and key representations, followed by scaling \n",
    "and softmax normalization. The attention scores are then used to compute context vectors for each head.\n",
    "\n",
    "3. **Concatenation and Linear Transformation**: The context vectors from all attention heads are concatenated\n",
    "along the last dimension and linearly transformed using another learnable weight matrix. This process allows \n",
    "the model to combine information from multiple attention heads and produce a richer representation of the \n",
    "input sequence.\n",
    "\n",
    "Mathematically, the computation of the multi-head attention mechanism can be summarized as follows:\n",
    "\n",
    "\\[\n",
    "\\text{{MultiHead}(Q, K, V)} = \\text{{Concat}}(\\text{{head}_1}, \\ldots, \\text{{head}_h}) \\cdot W^O\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( Q \\), \\( K \\), and \\( V \\) are the input query, key, and value representations, respectively,\n",
    "- \\(\\text{{head}_i} = \\text{{Attention}}(QW^Q_i, KW^K_i, VW^V_i)\\) represents the computation of the \\( i \\)-th \n",
    "attention head,\n",
    "- \\( W^Q_i \\), \\( W^K_i \\), and \\( W^V_i \\) are learnable weight matrices for the \\( i \\)-th attention head,\n",
    "- \\( W^O \\) is the learnable weight matrix used to linearly transform the concatenated outputs of all attention \n",
    "heads.\n",
    "\n",
    "Multi-head attention allows the model to attend to different parts of the input sequence simultaneously and \n",
    "learn diverse representations of the input. This enhances the model's ability to capture complex patterns and \n",
    "dependencies in the data, leading to improved performance in various natural language processing tasks, such \n",
    "as machine translation, text generation, and language understanding.\"\"\"\n",
    "\n",
    "#10. What is Bilingual Evaluation Understudy (BLEU)\n",
    "\n",
    "\"\"\"Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-generated\n",
    "translations by comparing them to reference translations. It was proposed by Kishore Papineni et al. in \n",
    "their paper \"BLEU: a Method for Automatic Evaluation of Machine Translation\" in 2002.\n",
    "\n",
    "BLEU operates by computing a score that measures the similarity between the candidate translation\n",
    "(the output of a machine translation system) and one or more reference translations (human-generated \n",
    "translations). The score is based on the precision of n-grams (contiguous sequences of n words) in the \n",
    "candidate translation compared to the reference translations.\n",
    "\n",
    "Here's how BLEU is calculated:\n",
    "\n",
    "1. **Compute N-gram Precision**: For each n-gram size \\( n \\) (typically ranging from 1 to 4), BLEU\n",
    "computes the precision of n-grams in the candidate translation compared to the reference translations. \n",
    "Precision is calculated as the ratio of the number of n-grams in the candidate translation that appear \n",
    "in any of the reference translations to the total number of n-grams in the candidate translation.\n",
    "\n",
    "2. **Brevity Penalty**: BLEU penalizes translations that are shorter than the reference translations to\n",
    "discourage overly concise translations. If the length of the candidate translation (\\( c \\)) is shorter \n",
    "than the length of the reference translation (\\( r \\)), a brevity penalty is applied to the BLEU score. \n",
    "The brevity penalty term is defined as \\( \\exp(1 - \\frac{r}{c}) \\).\n",
    "\n",
    "3. **Compute BLEU Score**: The overall BLEU score is computed as the geometric mean of the n-gram precisions,\n",
    "weighted by their respective lengths, and multiplied by the brevity penalty:\n",
    "\n",
    "\\[\n",
    "\\text{BLEU} = \\text{BP} \\times \\exp\\left( \\frac{1}{N} \\sum_{n=1}^{N} \\log p_n \\right)\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( p_n \\) is the precision of n-grams,\n",
    "- \\( N \\) is the maximum n-gram size considered,\n",
    "- \\( \\text{BP} \\) is the brevity penalty.\n",
    "\n",
    "BLEU scores range from 0 to 1, with higher scores indicating better translation quality. A perfect \n",
    "translation would achieve a BLEU score of 1.\n",
    "\n",
    "BLEU is widely used in the machine translation community as a quick and automatic way to evaluate the \n",
    "quality of machine-generated translations. However, it has certain limitations, such as its reliance on \n",
    "exact matches of n-grams and insensitivity to semantic similarity and fluency. Therefore, BLEU scores\n",
    "should be interpreted with caution and complemented with other evaluation methods for a comprehensive \n",
    "assessment of translation quality.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
